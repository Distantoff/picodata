# Создание кластера

В данном разделе приведена информация по развертыванию кластера Picodata
из нескольких инстансов, запущенных на одном или разных серверах. Запуск
отдельного инстанса описан в разделе [Запуск Picodata](run.md).

## Кластер из нескольких инстансов {: #distributed_cluster }

Разворачивание кластера из нескольких инстансов отражает сценарии
полноценного использования Picodata. Данная процедура аналогична как для
"локальных" кластеров (на одном хосте), так и для варианта с несколькими
серверами. В последнем случае  нужно будет в примерах ниже заменить
`127.0.0.1` на IP-адресы соответствующих серверов.

Для примера мы запустим на одном хосте кластер из 4-х инстансов. Для
этого потребуется заранее подготовить:

- [файл конфигурации][config] (одинаковый для вcех инстансов)
- параметры запуска инстанса (индивидуальны для каждого)

Файл конфигурации удобно подготовить на основе шаблона, созданного
командой `picodata config default`. По умолчанию файл содержит все
параметры [picodata run][picodata_run], что избыточно для тестового
примера. Сократите его до более компактного вида, например:

```yml
cluster:
  cluster_id: my_cluster
  tier:
    default:
      replication_factor: 2
      can_vote: true

instance:
  peer:
  - 127.0.0.1:3301
  audit: false
  shredding: false
  tier: default
  log:
    level: info
    format: plain
    destination: null
  memtx:
    memory: 67108864
    checkpoint_count: 2
    checkpoint_interval: 3600.0
  vinyl:
    memory: 134217728
    cache: 134217728
```

[picodata_run]: ../reference/cli.md#run

Такой набор настроек явно задает имя кластера (`cluster_id`), имя [тира][tier]
(`default`) и фактор репликации (`2`) для входящих в него репликасетов,
а также ряд параметров, универсальных для всех инстансов.

!!! note "Примечание"
    Если не использовать параметр `cluster_id`, то по
    умолчанию кластер будет носить имя demo.

Обратите внимание, что все инстансы будут использовать одно и то же
значение параметрам `peer`. Он нужен для того, чтобы дать инстансам
возможность обнаружить друг друга и позволить механизму [discovery]
правильно собрать все найденные экземпляры Picodata в один кластер.
Укажите в `peer` адрес какого-либо соседнего инстанса (в примере указан
первый). Без явного указания параметра `peer` каждый инстанс образует
свой независимый кластер.

Индивидуальные настройки инстансов включают:

- директорию для хранения [рабочих файлов][files] ([data-dir])
- адрес приема соединений ([listen])
- адрес HTTP-сервера ([http-listen])
- адрес подключения для клиентов PostgreSQL ([pg-listen])

!!! note "Примечание"
    Адрес HTTP-сервера и сервера для подключения клиента PostgreSQL
    достаточно указать только для первого инстанса в кластере

[data-dir]: ../reference/cli.md#run_data_dir
[listen]: ../reference/cli.md#run_listen
[http-listen]: ../reference/cli.md#run_http_listen
[pg-listen]: ../reference/cli.md#run_pg_listen

Эти настройки задаются как в виде аргументов для `picodata run`, так
и с помощью экспорта соответствующих переменных. Последний способ
позволяет более удобно управлять настройками в Bash-скрипте. Ниже
приведен пример скрипта, который задействует файл конфигурации
(`my_cluster.yml`), определяет индивидуальные настройки первого инстанса
(`i1`) и запускает его:

!!! example "i1"
  ```shell
  #!/bin/bash

  export PICODATA_CONFIG_FILE="my_cluster.yml"

  export PICODATA_INSTANCE_ID="i1"
  export PICODATA_DATA_DIR="tmp/i1"
  export PICODATA_LISTEN="127.0.0.1:3301"
  export PICODATA_HTTP_LISTEN="127.0.0.1:8080"
  export PICODATA_PG_LISTEN="127.0.0.1:5432"

  exec picodata run
  ```

Для параметра `listen` (переменной `PICODATA_LISTEN`) формат адреса допускает
упрощения — можно указать только хост `127.0.0.1` (порт по умолчанию
`:3301`), или только порт, но для наглядности лучше используйте полный
формат `<HOST>:<PORT>`.

[files]: ../architecture/instance_runtime_files.md
[cli]: ../reference/cli.md
[config]: ../reference/config.md
[tier]: ../overview/glossary.md#tier

Примеры скриптов для остальных трех инстансов:

??? example "i2"
    ```
    #!/bin/bash

    export PICODATA_CONFIG_FILE="my_cluster.yml"

    export PICODATA_INSTANCE_ID="i2"
    export PICODATA_DATA_DIR="tmp/i2"
    export PICODATA_LISTEN="127.0.0.1:3302"
    # export PICODATA_HTTP_LISTEN="127.0.0.1:8080"
    # export PICODATA_PG_LISTEN="127.0.0.1:5432"

    exec picodata run
    ```

??? example "i3"
    ```
    #!/bin/bash

    export PICODATA_CONFIG_FILE="my_cluster.yml"

    export PICODATA_INSTANCE_ID="i3"
    export PICODATA_DATA_DIR="tmp/i3"
    export PICODATA_LISTEN="127.0.0.1:3303"
    # export PICODATA_HTTP_LISTEN="127.0.0.1:8080"
    # export PICODATA_PG_LISTEN="127.0.0.1:5432"

    exec picodata run
    ```

??? example "i4"
    ```
    #!/bin/bash

    export PICODATA_CONFIG_FILE="my_cluster.yml"

    export PICODATA_INSTANCE_ID="i4"
    export PICODATA_DATA_DIR="tmp/i4"
    export PICODATA_LISTEN="127.0.0.1:3304"
    # export PICODATA_HTTP_LISTEN="127.0.0.1:8080"
    # export PICODATA_PG_LISTEN="127.0.0.1:5432"

    exec picodata run
    ```

Все корректно запущенные инстансы используют алгоритм [discovery] для
получения от raft информации о текущем лидере raft-группы и
автоматически добавляются в один кластер.

[discovery]: ../architecture/discovery.md

## Именование инстансов {: #instance_naming }

Чтобы проще было отличать инстансы друг от друга, задайте им имена.

При запуске с указанием параметров вручную:

```shell
picodata run --instance-id barsik
```

При запуске из скрипта, объявите в нем соответствующую переменную:

```shell
export PICODATA_INSTANCE_ID="barsik"
```

Если имя не дать, то оно будет сгенерировано автоматически в момент
добавления в кластер. Имя инстанса задается один раз и не может быть
изменено в дальнейшем (например, оно постоянно сохраняется в снапшотах
инстанса). В кластере нельзя иметь два инстанса с одинаковым именем —
пока инстанс живой, другой инстанс сразу после запуска получит ошибку
при добавлении в кластер. Тем не менее, имя можно повторно использовать,
если предварительно исключить первый инстанс с таким именем из кластера.
<!-- Это делается командой `picodata expel barsik`. -->

## Репликация и зоны доступности (failure domains) {: #failure_domains }

### Установка фактора репликации {: #replication_factor}

При создании кластера [количество реплик (инстансов)][rep_factor] в
репликасете определяется параметром запуска [--init-replication-factor]
или переменной `PICODATA_INIT_REPLICATION_FACTOR`. Указанное значение
присваивается всему кластеру при запуске первого инстанса и затем
хранится в виде параметра `replication_factor` в [файле
конфигурации][config].

Изменить фактор репликации для уже созданного
кластера нельзя.

[--init-replication-factor]: ../reference/cli.md#run_init_replication_factor
[--failure-domain]: ../reference/cli.md#run_failure_domain
[--advertise]: ../reference/cli.md#run_advertise
[rep_factor]: ../overview/glossary.md#replication_factor

<!-- Отредактировать фактор репликации, сохраненный в конфигурации кластера, можно командой `picodata set-replication-factor`. Редактирование конфигурации сказывается только на вновь добавляемых инстансах, но не затрагивает уже работающие. -->

### Использование зон доступности {: #setting_failure_domain}

По мере усложнения топологии возникает еще один вопрос — как не
допустить объединения в репликасет инстансов из одного и того же
датацентра. Для этого в Picodata имеется параметр [--failure-domain]
(переменная `PICODATA_FAILURE_DOMAIN`) — _зона доступности_, отражающая
признак физического размещения сервера, на котором выполняется инстанс
Picodata. Это может быть как датацентр, так и какое-либо другое
обозначение расположения: регион (например, `eu-east`), стойка, сервер,
или собственное обозначение (blue, green, yellow). Ниже показан пример
запуска инстанса Picodata в командной строке с указанием зоны
доступности:

```shell
picodata run --init-replication-factor 2 --failure-domain region=us,zone=us-west-1
```

С учетом этого параметра добавление инстанса в репликасет происходит так:

- если в каком-либо репликасете количество инстансов меньше необходимого
  фактора репликации, то новый инстанс добавляется в него при условии,
  что их параметры [--failure-domain] отличаются (регистр символов не
  учитывается)
- если подходящих репликасетов нет, то Picodata создает новый
  репликасет

Параметр [--failure-domain] играет роль только в момент добавления
инстанса в кластер. Принадлежность инстанса репликасету впоследствии не
меняется.

Как и параметр [--advertise], значение параметра [--failure-domain]
каждого инстанса можно редактировать, перезапустив инстанс с новыми
параметрами.

Добавляемый инстанс должен обладать, как минимум, тем же набором
параметров, которые уже есть в кластере. Например, инстанс `dc=msk` не
сможет присоединиться к кластеру с `--failure-domain region=eu/us` и
вернет ошибку.

Как было указано выше, сравнение зон доступности производится без учета
регистра символов, поэтому, к примеру, два инстанса с аргументами
`--failure-domain region=us` и `--failure-domain REGION=US` будут
относиться к одному региону и, следовательно, не попадут в один
репликасет.

## Удаление инстанса (expel) {: #expel }

Данная процедура позволяет исключить инстанс из состава кластера.

Если инстанс хранит сегменты шардированных данных, перед его удалением
данные будет автоматически перераспределены.

Для удаления инстанса из кластера потребуется пароль Администратора СУБД
(`admin`), который должен быть заранее установлен в консоли администратора:

```shell
picodata admin ./admin.sock
ALTER USER "admin" WITH PASSWORD 'T0psecret'
```

Для удаления инстанса из кластера используйте команду [picodata
expel](../reference/cli.md#expel):

```shell
picodata expel barsik --peer 192.168.0.1:3301
```

См. также:

- [Подключение и работа в консоли](connecting.md)
