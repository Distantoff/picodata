# Кластер на физических узлах

В данном разделе приведена информация по развертыванию кластера Picodata
на физических узлах. Данная информация применима также к узлам в
виртуальной среде.


## Минимальный вариант кластера {: #minimal-cluster }

Запуск кластера сводится к выполнению команды `picodata run` с нужным
набором параметров для каждого инстанса (узла). Полный перечень
возможных параметров запуска и их описание содержатся в разделе
[Аргументы командной строки Picodata](../cli), а также в выводе команды
`picodata run --help`. С точки зрения внутренней архитектуры, [_кластер_](glossary.md#cluster)
  корректно называть _raft-группой_ — в дальнейшем при мониторинге и
управлении конфигурацией будет уместнее использовать именно этот термин.

Picodata может создать кластер, состоящий всего из одного
экземпляра/инстанса. Обязательных параметров у него нет, что позволяет
свести запуск к выполнению всего одной простой команды:

```
picodata run
```

Можно добавлять сколько угодно последующих инcтансов — все они будут
подключаться к этому кластеру. Каждому инстансу следует задать отдельную
рабочую директорию (параметр `--data-dir`), а также указать адрес и порт
для приема соединений (параметр `--listen`) в формате `<HOST>:<PORT>`.
Фактор репликации по умолчанию равен 1 — каждый инстанс образует
отдельный репликасет. Если для `--listen` указать только порт, то будет
использован IP-адрес по умолчанию (127.0.0.1):

```
picodata run --data-dir i1 --listen :3301
picodata run --data-dir i2 --listen :3302
picodata run --data-dir i3 --listen :3303
```

Если не использовать параметр `cluster-id`, то по умолчанию кластер будет носить имя `demo`.

## Подключение к кластеру {: #accessing-console }

По умолчанию команда `picodata run` запускает инстанс, но не
предоставляет доступ к управлению им в консоли. Для подключения к
интерактивной консоли следует либо запускать инстанс с ключом `-i`
(`picodata run -i`), либо отдельно подключаться к инстансу командой
`picodata connect` (см. [подробнее](cli.md#connect-command)).

## Кластер на нескольких серверах {: #distributed-cluster }

Выше был показан запуск Picodata на одном сервере, что удобно для
тестирования и отладки, но не отражает сценариев полноценного
использования кластера. Поэтому ниже будет показан запуск Picodata на
нескольких серверах. Предположим, что их два: `192.168.0.1` и
`192.168.0.2`. Порядок действий будет следующим:

На `192.168.0.1`:
```shell
picodata run --listen 192.168.0.1:3301 --peer 192.168.0.1:3301
```

На `192.168.0.2`:
```shell
picodata run --listen 192.168.0.2:3301 --peer 192.168.0.1:3301
```

На что нужно обратить внимание:

Во-первых, для параметра `--listen` вместо стандартного значения `127.0.0.1` надо указать конкретный адрес. Формат адреса допускает упрощения — можно указать только хост `192.168.0.1` (порт по умолчанию `:3301`), или только порт, но для наглядности лучше использовать полный формат `<HOST>:<PORT>`.

Значение параметра `--listen` не хранится в кластерной конфигурации и может меняться при перезапуске инстанса.

Во-вторых, надо дать инстансам возможность обнаружить друг друга для того, чтобы механизм discovery правильно собрал все найденные экземпляры Picodata в один кластер. Для этого в параметре `--peer` нужно указать адрес какого-либо соседнего инстанса. По умолчанию значение параметра `--peer` установлено в `127.0.0.1:3301`. Параметр `--peer` не влияет больше ни на что, кроме механизма обнаружения других инстансов.

Параметр `--advertise` используется для установки публичного IP-адреса и порта инстанса. Параметр сообщает, по какому адресу остальные инстансы должны обращаться к текущему. По умолчанию он равен `--listen`, поэтому в примере выше не упоминается. Но, например, в случае `--listen 0.0.0.0` его придется указать явно:

```shell
picodata run --listen 0.0.0.0:3301 --advertise 192.168.0.1:3301
```

Значение параметра `--advertise` анонсируется кластеру при запуске инстанса. Его можно поменять при перезапуске инстанса или в процессе его работы командой `picodata set-advertise`.

## Именование инстансов {: #instance-naming }

Чтобы проще было отличать инстансы друг от друга, им можно давать имена:

```
picodata run --instance-id barsik
```

Если имя не дать, то оно будет сгенерировано автоматически в момент добавления в кластер. Имя инстанса задается один раз и не может быть изменено в дальнейшем (например, оно постоянно сохраняется в снапшотах инстанса). В кластере нельзя иметь два инстанса с одинаковым именем — пока инстанс живой, другой инстанс сразу после запуска получит ошибку при добавлении в кластер. Тем не менее, имя можно повторно использовать, если предварительно исключить первый инстанс с таким именем из кластера. <!-- Это делается командой `picodata expel barsik`. -->

<!--
## Группы и роли

До сих пор рассматриваемый кластер был гомогенным. Все инстансы были одинаковы по функциональности — хранили данные, обрабатывали запросы. В промышленной эксплуатации эти роли почти всегда требуется разделять, чтобы эффективнее использовать ресурсы оборудования. Под хранение выделяются серверы с большим объемом памяти, для обработки запросов это не требуется.

В Picodata для этих целей служит понятие групп инстансов. Принадлежность инстанса той или иной группе задается при добавлении в кластер параметром `--group` и впоследствии не может быть изменена. По умолчанию кластер состоит из одной группы "common".

Функциональность инстансов определяется набором ролей. На данный момент существует две роли:

- storage — позволяет хранить шардированные данные на инстансе.
- router — реализует логику доступа к шардированным данным.

По умолчанию инстанс исполняет обе роли одновременно, но его можно ограничить явным указанием одной из них:

```
picodata run --role storage
picodata run --role router
```

Важно то, что обе эти роли относятся только к шардированию. Так, отключение роли storage ничем не мешает хранить данные локально.

Также инстанс можно запустить без ролей вовсе, в результате чего он будет функционировать исключительно как не-шардированное локальное хранилище:

```
picodata run --no-role
```

Все инстансы в группе имеют одинаковый набор ролей и одинаковый фактор репликации.
-->


## Репликация и зоны доступности (failure domains) {: #failure-domains }

Количество инстансов в репликасете определяется значением переменной `replication_factor`. Внутри <!-- группы инстансов --> кластера используется один и тот же `replication_factor`.

Управление количеством происходит через параметр `--init-replication-factor`, который используется только в момент <!-- создания группы (добавления первого инстанса) --> запуска первого инстанса. При этом значение из аргументов командной строки записывается в конфигурацию кластера. В дальнейшем значение параметра `--init-replication-factor` игнорируется.

<!-- Отредактировать фактор репликации, сохраненный в конфигурации кластера, можно командой `picodata set-replication-factor`. Редактирование конфигурации сказывается только на вновь добавляемых инстансах, но не затрагивает уже работающие. -->

По мере усложнения топологии возникает еще один вопрос — как не допустить объединения в репликасет инстансов из одного и того же датацентра. Для этого в Picodata имеется параметр `--failure-domain` — _зона доступности_, отражающая признак физического размещения сервера, на котором выполняется инстанс Picodata. Это может быть как датацентр, так и какое-либо другое обозначение расположения: регион (например, `eu-east`), стойка, сервер, или собственное обозначение (blue, green, yellow). Ниже показан пример запуска инстанса Picodata с указанием зоны доступности:

```
picodata run --init-replication-factor 2 --failure-domain region=us,zone=us-west-1
```

Добавление инстанса в репликасет происходит по следующим правилам:

- Если в каком-либо репликасете количество инстансов меньше необходимого фактора репликации, то новый инстанс добавляется в него при условии, что их параметры `--failure-domain` отличаются (регистр символов не учитывается).
- Если подходящих репликасетов нет, то Picodata создает новый репликасет.

Параметр `--failure-domain` играет роль только в момент добавления инстанса в кластер. Принадлежность инстанса репликасету впоследствии не меняется.

Как и параметр `--advertise`, значение параметра`--failure-domain` каждого инстанса можно редактировать, перезапустив инстанс с новыми параметрами.

Добавляемый инстанс должен обладать тем же набором параметров, которые уже есть в кластере. Например, инстанс `dc=msk` не сможет присоединиться к кластеру с `--failure-domain region=eu/us` и вернет ошибку.

Как было указано выше, сравнение зон доступности производится без учета
регистра символов, поэтому, к примеру, два инстанса с аргументами
`--failure-domain region=us` и `--failure-domain REGION=US` будут относиться
к одному региону и, следовательно, не попадут в один репликасет.

Добавляемый инстанс использует алгоритм `discovery` для получения от raft информации о текущем лидере raft-группы.
<!-- исключения описаны ниже -->

<!--
## Кейс: два датацентра по две реплики

Picodata старается не объединять в один репликасет инстансы, у которых совпадает хотя бы один домен. Но иногда это все же необходимо. Чтобы ограничить Picodata в бесконечном создании репликасетов, можно воспользоваться флагом `--max-replicaset-count` (по умолчанию `inf`).

Как и `--init-replication-factor`, параметр `--max-replicaset-count` может быть разным для разных групп.

Как и другие параметры, `--max-replicaset-count` редактируется в любой момент:

- При добавлении нового инстанса
- В процессе его работы командой `picodata set-max-replicaset-count`

Важно учитывать, что параметр `--max-replicaset-count` нельзя сделать меньше существующего количества репликасетов.

## Файлы конфигурации

Существует три способа передать Picodata параметры конфигурации. Они приведены ниже в порядке возрастания приоритета:

1. Файл конфигурации (yaml / toml)
2. Переменные окружения `PICODATA_<PARAM>=<VALUE>`
3. Аргументы командной строки `--param value`

Мы перечислили достаточно много разнообразных параметров, некоторые из которых делают команду запуска достаточно длинной. Вместо отдельных команд можно использовать файл конифгурации. Пример:

<h5 a><strong><code>storage.toml</code></strong></h5>

```toml
group = "storages"
max-replicaset-count = 30
replication-factor = 4
roles = ["storage"]
```

<h5 a><strong><code>storage.toml</code></strong></h5>

```toml
group = "routers"
roles = ["router"]
```

Пример запуска кластера Picodata c использованием файла конфигурации:

```
picodata run --cfg storage.toml --listen :3301
picodata run --cfg router.toml --listen :3302
```
-->

## Удаление инстансов из кластера (expel) {: #instance-expulsion }

Удаление — это принятие кластером решения, что некий инстанс больше не является участником кластера. После удаления кластер больше не будет ожидать присутствия инстанса в кворуме, а сам инстанс завершится. При удалении текущего лидера будет принудительно запущен выбор нового лидера.

### Удаление инстанса с помощью консольной команды {: #expulsion-command}

```bash
picodata expel --instance-id <instance-id> [--cluster-id <cluster-id>] [--peer <peer>]
```

где `cluster-id` и `instance-id` — данные об удаляемом инстансе, `peer` — любой инстанс кластера.

Пример:

```bash
picodata expel --instance-id i3 --peer 192.168.100.123
```

В этом случае на адрес `192.168.100.123:3301` будет отправлена команда `expel` с `instance-id = "i3"` и стандартным значением `cluster-id`. Инстанс на `192.168.100.123:3301` найдет лидера и отправит ему команду `expel`. Лидер отметит, что указанный инстанс удален; остальные инстансы получат эту информацию через Raft. Если удаляемый инстанс запущен, он завершится, если не запущен — примет информацию о своем удалении при запуске и затем завершится. При последующих запусках удаленный инстанс будет сразу завершаться.

### Удаление инстанса из консоли Picodata с помощью Lua API {: #expulsion-lua-api }

В консоли запущенного инстанса введите следующее:

```lua
pico.expel(<instance-id>)
```

например:

```lua
pico.expel("i3")
```

Будет удален инстанс `i3`. Сам инстанс `i3` будет завершен. Если вы находитесь в консоли удаляемого инстанса — процесс завершится, консоль будет закрыта.


См. [отдельный подраздел](../raft_voters) с описанием работы алгоритма Raft для обеспечения отказоустойчивости кластера.

---
[Исходный код страницы](https://git.picodata.io/picodata/picodata/docs/-/blob/main/docs/deploy_on_hosts.md)
