# Развертывание кластера

В данном разделе приведена информация по созданию кластера Picodata и
описаны принципы его работы.


## Минимальный вариант кластера

Запуск кластера сводится к выполнению команды `picodata run` с нужным
набором параметров для каждого инстанса (узла). Полный перечень
возможных параметров запуска и их описание содержатся в подразделе
[Описание параметров запуска](../cli), а также в выводе команды
`picodata run --help`. С точки зрения внутренней архитектуры, _кластер_
  корректно называть _raft-группой_ — в дальнейшем при мониторинге и
управлении конфигурацией будет уместнее использовать именно этот термин.

Picodata может создать кластер, состоящий всего из одного
экземпляра/инстанса. Обязательных параметров у него нет, что позволяет
свести запуск к выполнению всего одной простой команды:

```
picodata run
```

Можно добавлять сколько угодно последующих инcтансов — все они будут
подключаться к этому кластеру. Каждому инстансу следует задать отдельную
рабочую директорию (параметр `--data-dir`), а также указать адрес и порт
для приема соединений (параметр `--listen`) в формате `<HOST>:<PORT>`.
Фактор репликации по умолчанию равен 1 — каждый инстанс образует
отдельный репликасет. Если для `--listen` указать только порт, то будет
использован IP-адрес по умолчанию (127.0.0.1):

```
picodata run --data-dir i1 --listen :3301
picodata run --data-dir i2 --listen :3302
picodata run --data-dir i3 --listen :3303
```

Если не использовать параметр `cluster-id`, то по умолчанию кластер будет носить имя `demo`.

## Кластер на нескольких серверах

Выше был показан запуск Picodata на одном сервере, что удобно для
тестирования и отладки, но не отражает сценариев полноценного
использования кластера. Поэтому ниже будет показан запуск Picodata на
нескольких серверах. Предположим, что их два: `192.168.0.1` и
`192.168.0.2`. Порядок действий будет следующим:

На `192.168.0.1`:
```shell
picodata run --listen 192.168.0.1:3301
```

На `192.168.0.2`:
```shell
picodata run --listen 192.168.0.2:3301 --peer 192.168.0.1:3301
```

На что нужно обратить внимание:

Во-первых, для параметра `--listen` вместо стандартного значения `127.0.0.1` надо указать конкретный адрес. Формат адреса допускает упрощения — можно указать только хост `192.168.0.1` (порт по умолчанию `:3301`), или только порт, но для наглядности лучше использовать полный формат `<HOST>:<PORT>`.

Значение параметра `--listen` не хранится в кластерной конфигурации и может меняться при перезапуске инстанса.

Во-вторых, надо дать инстансам возможность обнаружить друг друга для того чтобы механизм discovery правильно собрал все найденные экземпляры Picodata в один кластер. Для этого в параметре `--peer` нужно указать адрес какого-либо соседнего инстанса. По умолчанию значение параметра `--peer` установлено в `127.0.0.1:3301`. Параметр `--peer` не влияет больше ни на что, кроме механизма обнаружения других инстансов.

Параметр `--advertise` используется для установки публичного IP-адреса и порта инстанса. Параметр сообщает, по какому адресу остальные инстансы должны обращаться к текущему. По умолчанию он равен `--listen`, поэтому в примере выше не упоминается. Но, например, в случае `--listen 0.0.0.0` его придется указать явно:

```shell
picodata run --listen 0.0.0.0:3301 --advertise 192.168.0.1:3301
```

Значение параметра `--advertise` анонсируется кластеру при запуске инстанса. Его можно поменять при перезапуске инстанса или в процессе его работы командой `picodata set-advertise`.

## Именование инстансов

Чтобы проще было отличать инстансы друг от друга, им можно давать имена:

```
picodata run --instance-id barsik
```

Если имя не дать, то оно будет сгенерировано автоматически в момент добавления в кластер. Имя инстанса задается один раз и не может быть изменено в дальнейшем (например, оно постоянно сохраняется в снапшотах инстанса). В кластере нельзя иметь два инстанса с одинаковым именем — пока инстанс живой, другой инстанс сразу после запуска получит ошибку при добавлении в кластер. Тем не менее, имя можно повторно использовать если предварительно исключить первый инстанс с таким именем из кластера. <!-- Это делается командой `picodata expel barsik`. -->

<!--
## Группы и роли

До сих пор рассматриваемый кластер был гомогенным. Все инстансы были одинаковы по функциональности — хранили данные, обрабатывали запросы. В промышленной эксплуатации эти роли почти всегда требуется разделять, чтобы эффективнее использовать ресурсы оборудования. Под хранение выделяются серверы с большим объемом памяти, для обработки запросов это не требуется.

В Picodata для этих целей служит понятие групп инстансов. Принадлежность инстанса той или иной группе задается при добавлении в кластер параметром `--group` и впоследствии не может быть изменена. По умолчанию кластер состоит из одной группы "common".

Функциональность инстансов определяется набором ролей. На данный момент существует две роли:

- storage — позволяет хранить шардированные данные на инстансе.
- router — реализует логику доступа к шардированным данным.

По умолчанию инстанс исполняет обе роли одновременно, но его можно ограничить явным указанием одной из них:

```
picodata run --role storage
picodata run --role router
```

Важно то, что обе эти роли относятся только к шардированию. Так, отключение роли storage ничем не мешает хранить данные локально.

Также инстанс можно запустить без ролей вовсе, в результате чего он будет функционировать исключительно как не-шардированное локальное хранилище:

```
picodata run --no-role
```

Все инстансы в группе имеют одинаковый набор ролей и одинаковый фактор репликации.
-->

## Мониторинг состояния кластера
Для мониторинга состояния кластера удобно использовать команды,
показывающие состояние raft-группы, отдельных инстансов и собранных из
них репликасетов.

### Подключение
Каждый инстанс Picodata — это отдельный процесс в ОС. Для его
диагностики удобно воспользоваться встроенной консолью, которая
автоматически открывается после запуска инстанса (`picodata run ...`).
Получить доступ к инстансу из другой консоли можно командой `picodata
connect <host:port>`.

### Получение лидера raft-группы
Узнать лидера raft-группы, а также ID и статус текущего инстанса:
```
pico.raft_status()
```

Пример вывода:
```
---
- term: 2
  leader_id: 1
  raft_state: Leader
  id: 1
...

```
### Получение состава raft-группы 
Просмотр состава raft-группы и данных инстансов:
```
box.space._pico_instance:fselect()
```
Пример вывода:
```
---
- - ​+-----------+--------------------------------------+-------+-------------+--------------------------------------+-------------+------------+--------------+
  - ​|instance_id|            instance_uuid             |raft_id|replicaset_id|           replicaset_uuid            |current_grade|target_grade|failure_domain|
  - ​+-----------+--------------------------------------+-------+-------------+--------------------------------------+-------------+------------+--------------+
  - ​|   "i1"    |"68d4a766-4144-3248-aeb4-e212356716e4"|   1   |    "r1"     |"e0df68c5-e7f9-395f-86b3-30ad9e1b7b07"|["Online",1] |["Online",1]|      {}      |
  - ​|   "i2"    |"24c4ac5f-4981-3441-879c-aee1edb608a6"|   2   |    "r1"     |"e0df68c5-e7f9-395f-86b3-30ad9e1b7b07"|["Online",1] |["Online",1]|      {}      |
  - ​|   "i3"    |"5d7a7353-3e82-30fd-af0d-261436544389"|   3   |    "r2"     |"eff4449e-feb2-3d73-87bc-75807cb23191"|["Online",1] |["Online",1]|      {}      |
  - ​|   "i4"    |"826cbe5e-6979-3191-9e22-e39deef142f0"|   4   |    "r2"     |"eff4449e-feb2-3d73-87bc-75807cb23191"|["Online",1] |["Online",1]|      {}      |
  - ​+-----------+--------------------------------------+-------+-------------+--------------------------------------+-------------+------------+--------------+
...

```

Можно отдельно посмотреть список репликасетов, их UUID и вес (т.е. ):
```
box.space._pico_replicaset:fselect()
```
Пример вывода:
```
---
- — ​+-------------+--------------------------------------+---------+------+
  - ​|replicaset_id|           replicaset_uuid            |master_id|weight|
  - ​+-------------+--------------------------------------+---------+------+
  - ​|    "r1"     |"e0df68c5-e7f9-395f-86b3-30ad9e1b7b07"|  "i1"   |  1   |
  - ​|    "r2"     |"eff4449e-feb2-3d73-87bc-75807cb23191"|  "i3"   |  1   |
  - ​+-------------+--------------------------------------+---------+------+
...

```
Таблицы выше позволяют узнать текущий и целевой уровень (`grade`)
каждого инстанса, а также вес (`weight`) репликасета. Уровни отражают
конфигурацию остальных инстансов относительно текущего, а вес
репликасета — его наполненность репликами согласно фактору репликации
(см. [подробнее](../clustering)). Вес репликасета определяет его
приоритет при распределении бакетов с данными.

### Получение версии схемы данных

Узнать текущую версию схему данных можно с помощью команды:
```
box.space._pico_property:get("current_schema_version")
---
- ['current_schema_version', 1] 
...
```

Каждое изменение схемы данных в кластере приводит к
увеличению этого номера. 

## Репликация и зоны доступности (failure domains)

Количество инстансов в репликасете определяется значением переменной `replication_factor`. Внутри <!-- группы инстансов --> кластера используется один и тот же `replication_factor`.

Управление количеством происходит через параметр `--init-replication-factor`, который используется только в момент <!-- создания группы (добавления первого инстанса) --> запуска первого инстанса. При этом, значение из аргументов командной строки записывается в конфигурацию кластера. В дальнейшем значение параметра `--init-replication-factor` игнорируется.

<!-- Отредактировать фактор репликации, сохраненный в конфигурации кластера, можно командой `picodata set-replication-factor`. Редактирование конфигурации сказывается только на вновь добавляемых инстансах, но не затрагивает уже работающие. -->

По мере усложнения топологии возникает еще один вопрос — как не допустить объединения в репликасет инстансов из одного и того же датацентра. Для этого в Picodata имеется параметр `--failure-domain` — _зона доступности_, отражающая признак физического размещения сервера, на котором выполняется инстанс Picodata. Это может быть как датацентр, так и какое-либо другое обозначение расположения: регион (например, `eu-east`), стойка, сервер, или собственное обозначение (blue, green, yellow). Ниже показан пример запуска инстанса Picodata с указанием зоны доступности:

```
picodata run --init-replication-factor 2 --failure-domain region=us,zone=us-west-1
```

Добавление инстанса в репликасет происходит по следующим правилам:

- Если в каком-либо репликасете количество инстансов меньше необходимого фактора репликации, то новый инстанс добавляется в него при условии, что их параметры `--failure-domain` отличаются (регистр символов не учитывается).
- Если подходящих репликасетов нет, то Picodata создает новый репликасет.

Параметр `--failure-domain` играет роль только в момент добавления инстанса в кластер. Принадлежность инстанса репликасету впоследствии не меняется.

Как и параметр `--advertise`, значение параметра`--failure-domain` каждого инстанса можно редактировать, перезапустив инстанс с новыми параметрами.

Добавляемый инстанс должен обладать тем же набором параметров, которые уже есть в кластере. Например, инстанс `dc=msk` не сможет присоединиться к кластеру с `--failure-domain region=eu/us` и вернет ошибку.

Как было указано выше, сравнение зон доступности производится без учета
регистра символов, поэтому, к примеру, два инстанса с аргументами
`--failure-domain region=us` и `--failure-domain REGION=US` будут относиться
к одному региону и, следовательно, не попадут в один репликасет.

Добавляемый инстанс использует алгоритм `discovery` для получения от raft информации о текущем лидере raft-группы.
<!-- исключения описаны ниже -->

<!--
## Кейс: два датацентра по две реплики

Picodata старается не объединять в один репликасет инстансы, у которых совпадает хотя бы один домен. Но иногда это все же необходимо. Чтобы ограничить Picodata в бесконечном создании репликасетов, можно воспользоваться флагом `--max-replicaset-count` (по умолчанию `inf`).

Как и `--init-replication-factor`, параметр `--max-replicaset-count` может быть разным для разных групп.

Как и другие параметры, `--max-replicaset-count` редактируется в любой момент:

- При добавлении нового инстанса
- В процессе его работы командой `picodata set-max-replicaset-count`

Важно учитывать, что параметр `--max-replicaset-count` нельзя сделать меньше существующего количества репликасетов.

## Файлы конфигурации

Существует три способа передать Picodata параметры конфигурации. Они приведены ниже в порядке возрастания приоритета:

1. Файл конфигурации (yaml / toml)
2. Переменные окружения `PICODATA_<PARAM>=<VALUE>`
3. Аргументы командной строки `--param value`

Мы перечислили достаточно много разнообразных параметров, некоторые из которых делают команду запуска достаточно длинной. Вместо отдельных команд можно использовать файл конифгурации. Пример:

<h5 a><strong><code>storage.toml</code></strong></h5>

```toml
group = "storages"
max-replicaset-count = 30
replication-factor = 4
roles = ["storage"]
```

<h5 a><strong><code>storage.toml</code></strong></h5>

```toml
group = "routers"
roles = ["router"]
```

Пример запуска кластера Picodata c использованием файла конфигурации:

```
picodata run --cfg storage.toml --listen :3301
picodata run --cfg router.toml --listen :3302
```
-->

## Динамическое переключение голосующих узлов в Raft (Raft voter failover)

Все узлы Raft в кластере делятся на два типа: голосующие (`voter`) и неголосующие (`learner`). За консистентность raft-группы отвечают только узлы первого типа. Для коммита каждой транзакции требуется собрать кворум из `N/2 + 1` голосующих узлов. Неголосующие узлы в кворуме не участвуют.

Чтобы сохранить баланс между надежностью кластера и удобством его эксплуатации, в Picodata предусмотрена удобная функция — динамическое переключение типа узлов. Если один из голосующих узлов становится недоступным или прекращает работу (что может нарушить кворум в Raft), то тип `voter` автоматически присваивается одному из доступных неголосующих узлов. Переключение происходит незаметно для пользователя.

Количество голосующих узлов в кластере не настраивается и зависит только от общего количества инстансов. Если инстансов 1 или 2, то голосующий узел один. Если инстансов 3 или 4, то таких узлов три. Для кластеров с 5 или более инстансами — пять голосующих узлов.

Подробнее о запуске Picodata в командной строке см. разделе [Описание параметров запуска](../cli)

## Удаление инстансов из кластера (expel)

Удаление — это принятие кластером решения, что некий инстанс больше не является участником кластера. После удаления кластер больше не будет ожидать присутствия инстанса в кворуме, а сам инстанс завершится. При удалении текущего лидера будет принудительно запущен выбор нового лидера.

### Удаление инстанса с помощью консольной команды

```bash
picodata expel --instance-id <instance-id> [--cluster-id <cluster-id>] [--peer <peer>]
```

где `cluster-id` и `instance-id` — данные об удаляемом инстансе, `peer` — любой инстанс кластера.

Пример:

```bash
picodata expel --instance-id i3 --peer 192.168.100.123
```

В этом случае на адрес `192.168.100.123:3301` будет отправлена команда `expel` с `instance-id = "i3"` и стандартным значением `cluster-id`. Инстанс на `192.168.100.123:3301` найдет лидера и отправит ему команду `expel`. Лидер отметит, что указанный инстанс удален; остальные инстансы получат эту информацию через Raft. Если удаляемый инстанс запущен, он завершится, если не запущен — примет информацию о своем удалении при запуске и затем завершится. При последующих запусках удаленный инстанс будет сразу завершаться.

### Удаление инстанса из консоли Picodata с помощью Lua API

В консоли запущенного инстанса введите следующее:

```lua
pico.expel(<instance-id>)
```

например:

```lua
pico.expel("i3")
```

Будет удален инстанс `i3`. Сам инстанс `i3` будет завершен. Если вы находитесь в консоли удаляемого инстанса — процесс завершится, консоль будет закрыта.


См. [отдельный подраздел](../api) с описанием Lua API в Picodata. Дополнительно о внутренней архитектуре кластера Picodata можно узнать в разделе [Общая схема инициализации кластера](../clustering).


---
[Исходный код страницы](https://git.picodata.io/picodata/picodata/docs/-/blob/main/docs/deploy.md)
